---
title: "pg_lm Simulation"
bibliography: pg.bib
output:
  bookdown::html_document2: default
  bookdown::pdf_document2: 
    keep_tex: true
nocite: '@*'  
---

# To Do

- Figure out issue with fast `rmvn_*()` functions
- add in Polya-gamma data augmentation to DAG

We are testing the Polya-gamma linear model `pg_lm()`

```{r setup, include=FALSE}
library(pgR)
# library(MCMCpack)
library(splines)
library(tidyverse)
library(patchwork)
library(BayesLogit)
```


## simulate some data
```{r}
set.seed(404)
N <- 5000
J <- 2
X <- runif(N)
df <- 4
Xbs <- bs(X, df)
beta <- matrix(rnorm((J-1) * df), df, (J-1))
## make the intercepts smaller to reduce stochastic ordering effect
beta[1, ] <- beta[1, ] - seq(from = 4, to = 0, length.out = J - 1)
eta <- Xbs %*% beta 
pi <- eta_to_pi(eta)

Y <- matrix(0, N, J)

for (i in 1:N) {
    Y[i, ] <- rmultinom(1, 1, pi[i, ])
}
Y_prop     <- counts_to_proportions(Y)
```

## Plot the simulated data

```{r}
dat <- data.frame(
  y        = c(Y_prop),
  
  x        = X,
  species  = factor(rep(1:J, each = N)))

dat_truth <- data.frame(
  y        = c(pi),
  x        = X,
  species  = factor(rep(1:J, each = N)))

dat %>%
  group_by(species) %>%
  sample_n(pmin(nrow(Y), 500)) %>%
  ggplot(aes(y = y, x = x, group = species, color = species)) +
  geom_point(alpha = 0.2) +
  ylab("Proportion of count") +
  geom_line(data = dat_truth, aes(y = y, x = x, group = species), color = "black", lwd = 0.5) +
  facet_wrap(~ species, ncol = 4) +
  ggtitle("Simulated data")
```






```{r, message = FALSE}
## fit the model to verify parameters
params <- default_params() 
params$n_adapt <- 1000
params$n_mcmc <- 1000
params$n_message <- 500
params$n_thin <- 1
priors <- default_priors_pg_lm(Y, Xbs)
inits <- default_inits_pg_lm(Y, Xbs, priors)
if (file.exists(here::here("results", "pg_logit.RData"))) {
    load(here::here("results", "pg_logit.RData"))
} else {
    start <- Sys.time()
    out <- pg_lm(Y, as.matrix(Xbs), params, priors, n_cores = 1L, sample_rmvn = FALSE)
    stop <- Sys.time()
    runtime <- stop - start

    save(out, runtime, file = here::here("results", "pg_logit.RData"))
}
```


```{r}
betas <- out$beta
dimnames(betas) <- list(
  iteration = 1:dim(betas)[1],
  parameter = 1:dim(betas)[2],
  species = 1:dim(betas)[3]
)
as.data.frame.table(betas, responseName = "value") %>%
  mutate(iteration = as.numeric(iteration)) %>%
  ggplot(aes(x = iteration, y = value, group = parameter, color = parameter)) +
  geom_line() +
  facet_wrap(~ species)

apply(out$beta, 2, mean)
beta
# layout(matrix(1:9, 3, 3))
# for (i in 1:9) {
#   matplot(out$beta[, , i], type = 'l', main = paste("species", i))
#   abline(h = beta[, i], col = 1:nrow(beta))
# }

etas <- out$eta
```


```{r}
## plot beta estimates
p_beta <- data.frame(
  beta_mean = c(apply(out$beta, c(2, 3), mean)), 
  beta_lower = c(apply(out$beta, c(2, 3), quantile, prob = 0.025)),  
  beta_upper = c(apply(out$beta, c(2, 3), quantile, prob = 0.975)), 
  beta_truth = c(beta),
  species = factor(rep(1:(J-1), each = ncol(Xbs))),
  knots = factor(1:ncol(Xbs))
) %>%
  ggplot(aes(x = beta_truth, y = beta_mean, color = knots)) +
  scale_color_viridis_d(begin = 0, end = 0.8) +
  geom_point() +
  geom_errorbar(aes(ymin = beta_lower, ymax = beta_upper)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~ species, nrow = 3) +
  geom_abline(intercept = 0, slope = 1, col = "red")

## plot eta estimates
p_eta <- data.frame(
  eta_mean = c(apply(out$eta, c(2, 3), mean)), 
  eta_lower = c(apply(out$eta, c(2, 3), quantile, prob = 0.025)),  
  eta_upper = c(apply(out$eta, c(2, 3), quantile, prob = 0.975)), 
  eta_truth = c(eta),
  species = factor(rep(1:(J-1), each = ncol(Xbs))),
  knots = factor(1:ncol(Xbs))
) %>%
  ggplot(aes(x = eta_truth, y = eta_mean, color = knots)) +
  scale_color_viridis_d(begin = 0, end = 0.8) +
  geom_point() +
  geom_errorbar(aes(ymin = eta_lower, ymax = eta_upper)) +
  geom_point(alpha = 0.5) +
  facet_wrap(knots ~ species, nrow = 3) +
  geom_abline(intercept = 0, slope = 1, col = "red")

p_beta / p_eta
```


```{r}
pi_post <- array(0, dim = c(dim(out$eta)[1], dim(out$eta)[2], dim(out$eta)[3] + 1))
for (i in 1:dim(out$eta)[1]) {
  pi_post[i, , ] <- eta_to_pi(out$eta[i, , ])
}

 dat_pi <- data.frame(
  pi_mean = c(apply(pi_post, c(2, 3), mean)), 
  pi_lower = c(apply(pi_post, c(2, 3), quantile, prob = 0.025)),  
  pi_upper = c(apply(pi_post, c(2, 3), quantile, prob = 0.975)), 
  pi_truth = c(pi),
  species = factor(rep(1:J, each = N)),
  observation = factor(1:N)) 
 
p_pi <-  dat_pi %>% 
  ggplot(aes(x = pi_truth, y = pi_mean)) +
  scale_color_viridis_d(begin = 0, end = 0.8) +
  geom_point() +
  geom_errorbar(aes(ymin = pi_lower, ymax = pi_upper)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~ species, nrow = 3) +
  geom_abline(intercept = 0, slope = 1, col = "red")

p_pi
```



```{r}
# plot the fitted response curves
dat <- data.frame(
  y        = c(Y_prop),
  
  x        = X,
  species  = factor(rep(1:J, each = N)))

dat_fit <- data.frame(
  pi_mean  = c(apply(pi_post, c(2, 3), mean)),
  pi_lower = c(apply(pi_post, c(2, 3), quantile, prob = 0.025)),  
  pi_upper = c(apply(pi_post, c(2, 3), quantile, prob = 0.975)), 
  x        = X,
  species  = factor(rep(1:J, each = N)))

dat %>%
  group_by(species) %>%
  sample_n(pmin(nrow(Y), 500)) %>%
  ggplot(aes(y = y, x = x, group = species, color = species)) +
  geom_point(alpha = 0.2) +
  ylab("Proportion of count") +
  geom_line(data = dat_fit, aes(y = pi_mean, x = x, group = species), color = "black", lwd = 0.5) +
  geom_line(data = dat_truth, aes(y = y, x = x, group = species), color = "blue", lwd = 0.5) +
  facet_wrap(~ species, ncol = 4) +
  ggtitle("Simulated data")
```




<!-- # Compare to BayesLogit -->

<!-- ```{r} -->

<!-- logit.R <- function(y, X, n=rep(1, length(y)), -->
<!--                     m0=rep(0, ncol(X)), P0=matrix(0, nrow=ncol(X), ncol=ncol(X)), -->
<!--                     samp=1000, burn=500, verbose=500) -->
<!-- { -->
<!--   ## X: n by p matrix -->
<!--   ## y: n by 1 vector, avg response -->
<!--   ## n: n by 1 vector, # of obs at distinct x -->

<!--   ## Combine data. -->
<!--   ## new.data = logit.combine(y, X, n); -->
<!--   ## y = new.data$y; -->
<!--   ## X = new.data$X; -->
<!--   ## n = new.data$n; -->
<!--   ## n.prior = 0.0; -->

<!--   X = as.matrix(X); -->
<!--   y = as.numeric(y) -->

<!--   p = ncol(X) -->
<!--   N = nrow(X) -->

<!--   alpha = (y-1/2)*n -->

<!--   Z = colSums(X*alpha) + P0 %*% m0; -->
<!--   ## PsiToBeta = solve(t(X) %*% X) %*% t(X); -->

<!--   w = rep(0,N) -->
<!--   ## w = w.known; -->
<!--   beta = rep(0.0, p) -->

<!--   output <- list(w = matrix(nrow=samp, ncol=N), -->
<!--                  beta = matrix(nrow=samp, ncol=p) -->
<!--                  ) -->

<!--   ## c_k = (1:200-1/2)^2 * pi^2 * 4; -->

<!--   ## Timing -->
<!--   start.time = proc.time() -->

<!--   ## Sample -->
<!--   for ( j in 1:(samp+burn) ) -->
<!--   { -->
<!--     if (j==burn+1) start.ess = proc.time(); -->

<!--     ## draw w -->
<!--     psi = drop(X%*%beta) -->
<!--     ## Sum of gamma: poor approximation when psi is large!  Causes crash. -->
<!--     ## w = rpg.gamma(N, n, psi) -->
<!--     ## Devroye is faster anyway. -->
<!--     w = rpg(N, n, psi); -->

<!--     ## draw beta - Joint Sample. -->
<!--     PP = t(X) %*% (X * w) + P0; -->
<!--     ## U = chol(PP); -->
<!--     ## m = backsolve(U, Z, transpose=TRUE); -->
<!--     ## m = backsolve(U, m); -->
<!--     ## beta = m + backsolve(U, rnorm(p)) -->
<!--     S = chol2inv(chol(PP)); -->
<!--     m = S %*% as.vector(Z); -->
<!--     beta = m + t(chol(S)) %*% rnorm(p); -->

<!--     # Record if we are past burn-in. -->
<!--     if (j>burn) { -->
<!--         output$w[j-burn,] <- w -->
<!--         output$beta[j-burn,] <- beta -->
<!--     } -->

<!--     if (j %% verbose == 0) { print(paste("LogitPG: Iteration", j)); } -->
<!--   } -->

<!--   end.time = proc.time() -->
<!--   output$total.time = end.time - start.time -->
<!--   output$ess.time   = end.time - start.ess -->

<!--   ## Add new data to output. -->
<!--   output$"y" = y; -->
<!--   output$"X" = X; -->
<!--   output$"n" = n; -->

<!--   output -->
<!-- } ## logit.gibbs.R -->
<!-- ``` -->


<!-- ```{r} -->
<!-- out_logit <- logit.R(y = Y[, 1], X = as.matrix(Xbs), n = rowSums(Y)) -->
<!-- ``` -->

<!-- ```{r, message = FALSE} -->
<!-- ## fit the model with a linear response only -->
<!-- params <- default_params()  -->
<!-- params$n_adapt <- 500 -->
<!-- params$n_mcmc <- 1000 -->
<!-- params$n_message <- 50 -->
<!-- params$n_thin <- 1 -->
<!-- priors <- default_priors_pg_lm(Y, as.matrix(X)) -->
<!-- inits <- default_inits_pg_lm(Y, as.matrix(X), priors) -->
<!-- if (file.exists(here::here("results", "pg_lm-linear.RData"))) { -->
<!--     load(here::here("results", "pg_lm-linear.RData")) -->
<!-- } else { -->
<!--     start <- Sys.time() -->
<!--     out_linear <- pg_lm(Y, as.matrix(X), params, priors, n_cores = 1L, sample_rmvn = FALSE) -->
<!--     stop <- Sys.time() -->
<!--     runtime_linear <- stop - start -->

<!--     save(out_linear, runtime_linear, file = here::here("results", "pg_lm-linear.RData")) -->
<!-- } -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # model fit using loo -->
<!-- ll_bs <- calc_ll_pg_lm(Y, Xbs, out) -->
<!-- # fit model using linear response -->
<!-- ll_linear <- calc_ll_pg_lm(Y, as.matrix(X), out_linear) -->

<!-- library(loo) -->
<!-- loo_bs <- loo(t(ll_bs$ll), cores = 32L) -->
<!-- loo_linear <- loo(t(ll_linear$ll), cores = 32L) -->
<!-- comp <- loo_compare(loo_bs, loo_linear) -->
<!-- print(comp) -->
<!-- plot(loo_bs) -->
<!-- plot(loo_linear) -->
<!-- ``` -->

<!-- ## prediction code testing -->

<!-- ```{r, message = FALSE} -->
<!-- set.seed(44) -->
<!-- ## subsample the simulated data -->
<!-- n <- 2000 -->
<!-- s <- sample(N, n) -->
<!-- Y_s     <- Y[s, ] -->
<!-- Y_oos   <- Y[-s, ] -->
<!-- Xbs_s   <- Xbs[s, ] -->
<!-- Xbs_oos <- Xbs[-s, ] -->
<!-- eta_s   <- eta[s, ] -->
<!-- eta_oos <- eta[-s, ] -->
<!-- pi_s    <- pi[s, ] -->
<!-- pi_oos  <- pi[-s, ] -->

<!-- ## fit the model to verify parameters -->
<!-- params <- default_params() -->
<!-- params$n_adapt <- 500 -->
<!-- params$n_mcmc <- 1000 -->
<!-- params$n_message <- 50 -->
<!-- params$n_thin <- 1 -->
<!-- priors <- default_priors_pg_lm(Y_s, Xbs_s) -->
<!-- inits <- default_inits_pg_lm(Y_s, Xbs_s, priors) -->
<!-- if (file.exists(here::here("results", "pg_lm-sample.RData"))) { -->
<!--     load(here::here("results", "pg_lm-sample.RData")) -->
<!-- } else { -->
<!--     ## need to parallelize the polya-gamma random variables -->
<!--     ## can I do this efficiently using openMP? -->
<!--     ## Q: faster to parallelize using openMP or foreach? -->
<!--     start <- Sys.time() -->
<!--     out <- pg_lm(Y_s, as.matrix(Xbs_s), params, priors, n_cores = 1L, sample_rmvn = FALSE) -->
<!--     stop <- Sys.time() -->
<!--     runtime <- stop - start -->

<!--     save(out, runtime, file = here::here("results", "pg_lm-sample.RData")) -->
<!-- } -->
<!-- ``` -->


<!-- ```{r} -->
<!-- layout(matrix(1:9, 3, 3)) -->
<!-- for (i in 1:9) { -->
<!--   matplot(out$beta[, , i], type = 'l', main = paste("species", i)) -->
<!--   abline(h = beta[, i]) -->
<!-- } -->
<!-- ``` -->


<!-- ```{r} -->
<!-- runtime -->

<!-- ## plot beta estimates -->
<!-- dat_plot <- data.frame( -->
<!--     beta = c( -->
<!--         c(apply(out$beta, c(2, 3), mean)),  -->
<!--         c(beta) -->
<!--     ), -->
<!--     type = rep(c("estimate", "truth"), each = (J-1) * ncol(Xbs)), -->
<!--     species = factor(rep(1:(J-1), each = ncol(Xbs))), -->
<!--     knots = 1:ncol(Xbs) -->
<!-- ) -->

<!-- dat_plot %>% -->
<!--     pivot_wider(names_from = type, values_from = beta) %>% -->
<!--     ggplot(aes(x = estimate, y = truth, color = species)) + -->
<!--     scale_color_viridis_d(begin = 0, end = 0.8) + -->
<!--     geom_point(alpha = 0.5) + -->
<!--     facet_wrap(~ species, nrow = 8) + -->
<!--     geom_abline(intercept = 0, slope = 1, col = "red") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ## predict from the model -->
<!-- preds <- predict_pg_lm(out, Xbs_oos) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- pi_pred_mean <- apply(preds$pi, c(2, 3), mean) -->
<!-- dat_pi_pred <- data.frame( -->
<!--   pi   = c( -->
<!--     c(pi_oos), -->
<!--     c(pi_pred_mean) -->
<!--   ), -->
<!--   type = rep(c("observed", "predicted"), each = J * (N-n)), -->
<!--   species = factor(rep(1:J, each = (N-n))), -->
<!--   obs     = 1:(N-n)  -->
<!-- ) -->

<!-- p_pi_pred <- dat_pi_pred %>% -->
<!--   pivot_wider(names_from = type, values_from = pi) %>% -->
<!--   ggplot(aes(x = observed, y = predicted, color = species)) + -->
<!--   scale_color_viridis_d(begin = 0, end = 0.8) + -->
<!--   geom_point(alpha = 0.5) + -->
<!--   facet_wrap(~ species, nrow = 5) + -->
<!--   geom_abline(intercept = 0, slope = 1, col = "red") + -->
<!--   ggtitle("Predicted vs simulated latent probability pi") -->


<!-- eta_pred_mean <- apply(preds$eta, c(2, 3), mean) -->
<!-- dat_eta_pred <- data.frame( -->
<!--   eta   = c( -->
<!--     c(eta_oos), -->
<!--     c(eta_pred_mean) -->
<!--   ), -->
<!--   type = rep(c("observed", "predicted"), each = (J-1) * (N-n)), -->
<!--   species = factor(rep(1:(J-1), each = (N-n))), -->
<!--   obs     = 1:(N-n)  -->
<!-- ) -->

<!-- p_eta_pred <- dat_eta_pred %>% -->
<!--   pivot_wider(names_from = type, values_from = eta) %>% -->
<!--   ggplot(aes(x = observed, y = predicted, color = species)) + -->
<!--   scale_color_viridis_d(begin = 0, end = 0.8) + -->
<!--   geom_point(alpha = 0.5) + -->
<!--   facet_wrap(~ species, nrow = 5) + -->
<!--   geom_abline(intercept = 0, slope = 1, col = "red") + -->
<!--   ggtitle("Predicted vs simulated latent intensity eta") -->
<!-- p_pi_pred + p_eta_pred -->
<!-- ``` -->

<!-- ## verifying openMP timings -->


<!-- ```{r, message = FALSE} -->
<!-- params <- default_params() -->
<!-- params$n_adapt <- 100 -->
<!-- params$n_mcmc <- 100 -->
<!-- params$n_message <- 50 -->
<!-- params$n_thin <- 1 -->
<!-- priors <- default_priors_pg_lm(Y, Xbs) -->
<!-- inits <- default_inits_pg_lm(Y, Xbs, priors) -->
<!-- # check_inits_pg_lm(Y, Xbs, params, inits) -->

<!-- if (file.exists(here::here("results", "timings-pg_lm.RData"))) { -->
<!--     load(here::here("results", "timings-pg_lm.RData")) -->
<!-- } else { -->
<!--   ## need to parallelize the polya-gamma random variables -->
<!--   ## can I do this efficiently using openMP? -->
<!--   ## Q: faster to parallelize using openMP or foreach? -->

<!--   time_8 <- system.time( -->
<!--     out <- pg_lm(Y, as.matrix(Xbs), params, priors, n_cores = 8L, sample_rmvn = FALSE) -->
<!--   ) -->
<!--   time_6 <- system.time( -->
<!--     out <- pg_lm(Y, as.matrix(Xbs), params, priors, n_cores = 6L, sample_rmvn = FALSE) -->
<!--   ) -->
<!--   time_4 <- system.time( -->
<!--     out <- pg_lm(Y, as.matrix(Xbs), params, priors, n_cores = 4L, sample_rmvn = FALSE) -->
<!--   ) -->
<!--   time_2 <- system.time( -->
<!--     out <- pg_lm(Y, as.matrix(Xbs), params, priors, n_cores = 2L, sample_rmvn = FALSE) -->
<!--   ) -->
<!--   time_1 <- system.time( -->
<!--     out <- pg_lm(Y, as.matrix(Xbs), params, priors, n_cores = 1L, sample_rmvn = FALSE) -->
<!--   ) -->
<!--   save(time_1, time_2, time_4, time_6, time_8, file = here::here("results", "timings-pg_lm.RData")) -->
<!-- } -->

<!-- ## Note: a large amount of computation time is not parallel -- sampling of the beta variable -->
<!-- ## need to figure out why the faster comptuation version is giving issues -->
<!-- knitr::kable( -->
<!--     rbind( -->
<!--         time_1, -->
<!--         time_2, -->
<!--         time_4, -->
<!--         time_6, -->
<!--         time_8 -->
<!--     ) -->
<!-- ) -->
<!-- ``` -->


<!-- ## code profiling -->

<!-- ```{r, eval = FALSE} -->
<!-- ## not run -- diagnostic profiling -->
<!-- params$n_adapt <- 50 -->
<!-- params$n_mcmc <- 50 -->
<!-- profvis::profvis(pg_lm_cores(Y, as.matrix(Xbs), params, priors, cores = 1L)) -->
<!-- profvis::profvis(pg_lm(Y, as.matrix(Xbs), params, priors, cores = 8L)) -->
<!-- ``` -->
